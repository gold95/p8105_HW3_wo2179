---
title: "Homework 3"
author: "Wuraola Olawole"
date: "10/8/2020"
output: github_document
---


```{r}
library(tidyverse)
library(readxl)
library(p8105.datasets)
library(viridis)
```

# Problem 1 
```{r}
data("instacart")

```

This dataset contains `r nrow(instacart)` rows and ... columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


Let's make a plot

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```


Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```

# problem 2
Read dataset and tidy! 
```{r}
accel_df =
          read_csv(
		                "./Data_set/accel_data.csv") %>% 
    janitor::clean_names() %>%
  pivot_longer(activity_1:activity_1440,
                   names_to = "activity", 
                   values_to = "activity_count") %>%
  separate(col = activity, into = c("activity", "minute"), sep = "_") %>%
    select(-activity) %>%
     mutate(
       minute = as.numeric(minute),
       day = as_factor(day), 
        day = fct_relevel(day, "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"),
       wkday_wkend = recode(day, Sunday = "weekend", Saturday = "weekend", .default = "weekday" ),
       ) %>%
  arrange(week,day)

```

```{r}
accel_df =
          accel_df %>%
            mutate(
                  day_id = cumsum(!duplicated(accel_df[1:2]))
  )
```

```{r}
  accel_df %>%
    group_by(week, day) %>%
      summarize(across(activity_count:minute, mean)) %>%
knitr::kable()
```
```{r}

accel_df %>%
ggplot( aes(x = minute , y = activity_count, color = day)) + 
  geom_point() + geom_line() +
  labs(
    title = "24 hour Activity count of an Accelerometer",
    x = "Minutes of the day",
    y = "Activity count",
    caption = "Data from an accelerometer"
  ) +
scale_color_viridis() + theme_bw() +
  viridis::scale_color_viridis(
    name = "Days of the week", 
    discrete = TRUE
  )
```

# Problem 3
Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?
```{r}

data("ny_noaa")
```

```{r}

ny_df = 
        ny_noaa %>%
           janitor::clean_names() %>%
  separate(col = date, into = c("year", "month", "day"), sep = "-") %>%
mutate(
  tmax = as.numeric(tmax), 
  tmin = as.numeric(tmin),
  pre
)
```

```{r}

ny_df %>%
  count(snow)
```

