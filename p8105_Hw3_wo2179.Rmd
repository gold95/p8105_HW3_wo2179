---
title: "Homework 3"
author: "Wuraola Olawole"
date: "10/8/2020"
output: github_document
---


```{r}
library(tidyverse)
library(readxl)
library(p8105.datasets)
library(viridis)
library(patchwork)
```

# Problem 1 
```{r}
data("instacart")

```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```
Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```

Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```
# problem 2

## Part a

Read dataset and tidy! 
```{r}
accel_df =
          read_csv(
		                "./Data_set/accel_data.csv") %>% 
    janitor::clean_names() %>%
  pivot_longer(activity_1:activity_1440,
                   names_to = "activity", 
                   values_to = "activity_count") %>%
  separate(col = activity, into = c("activity", "minute"), sep = "_") %>%
    select(-activity) %>%
     mutate(
       minute = as.numeric(minute),
       day = as_factor(day), 
        day = fct_relevel(day, "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"),
       wkday_wkend = recode(day, Sunday = "weekend", Saturday = "weekend", .default = "weekday" ),
       ) %>%
  arrange(week,day)

```

```{r}
accel_df =
          accel_df %>%
            mutate(
                  day_id = cumsum(!duplicated(accel_df[1:2]))
  )

accel_df
```

The resulting dataset has `r nrow (accel_df)` rows and `r ncol(accel_df)` columns There are `r ncol(accel_df)` variables in this dataset. This dataset contains a recorded data of the activity count of a 63 year old man on an accelerometer over 24 hour period (measured in minutes) over 7 days and 5 weeks in all. The observations made in this data corresponds to the activity count per minutes of the day over 35 days.

## Part b

```{r}
  accel_df %>%
    group_by(week, day) %>%
      summarize(across(activity_count:minute, mean)) %>%
knitr::kable()
```

From this table, it can be observed that the activity count are never constant, and there is more activity count on sundays and certain days of the week. The least activity count seem to be observed on saturdays. In some weeks he is more active than in others.

## Part c

```{r}

accel_df %>%
ggplot( aes(x = minute , y = activity_count, color = day)) + 
  geom_point() + geom_line() +
  labs(
    title = "24 hour Activity count of an Accelerometer",
    x = "Minutes of the day",
    y = "Activity count",
    caption = "Data from an accelerometer"
  ) +
scale_color_viridis() + theme_bw() +
  viridis::scale_color_viridis(
    name = "Days of the week", 
    discrete = TRUE
  )
```

From this plot, the activity count begins low (midnight) and increases as the day progresses. It ebbs later in the day (towards night time). It can also be observed that as a whole, he is more active on sundays and certain weekdays. Across the weeks, he is consistently least active on saturdays.


# Problem 3

Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units.

## Part a

```{r}

data("ny_noaa")
```

```{r}

ny_df =
  ny_noaa %>%
    janitor::clean_names() %>%
      separate(col = date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(
          tmax = as.numeric(tmax),
            tmax = tmax / 10 ,
              tmin = as.numeric(tmin),
                tmin = tmin / 10,
                  prcp = prcp / 10
)
```

The most common values of snowfall are ` r ny_df %>% count()` because these are the frequently observed values across most of the stations within the same time frame.

